{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=False, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=False, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        output = F.softmax(self.layers(x))  \n",
    "        return output\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.load_state_dict(torch.load('mnist.pth'))\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/torch-1.7-py3/lib/python3.7/site-packages/ipykernel/__main__.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.278233\n",
      "Epoch: 2 \tTraining Loss: 1.978062\n",
      "Epoch: 3 \tTraining Loss: 1.695597\n",
      "Epoch: 4 \tTraining Loss: 1.649341\n",
      "Epoch: 5 \tTraining Loss: 1.635431\n",
      "Epoch: 6 \tTraining Loss: 1.628009\n",
      "Epoch: 7 \tTraining Loss: 1.623096\n",
      "Epoch: 8 \tTraining Loss: 1.619445\n",
      "Epoch: 9 \tTraining Loss: 1.616527\n",
      "Epoch: 10 \tTraining Loss: 1.614068\n",
      "Epoch: 11 \tTraining Loss: 1.611922\n",
      "Epoch: 12 \tTraining Loss: 1.610011\n",
      "Epoch: 13 \tTraining Loss: 1.608284\n",
      "Epoch: 14 \tTraining Loss: 1.606704\n",
      "Epoch: 15 \tTraining Loss: 1.605242\n",
      "Epoch: 16 \tTraining Loss: 1.603865\n",
      "Epoch: 17 \tTraining Loss: 1.602557\n",
      "Epoch: 18 \tTraining Loss: 1.601310\n",
      "Epoch: 19 \tTraining Loss: 1.600122\n",
      "Epoch: 20 \tTraining Loss: 1.598989\n",
      "Epoch: 21 \tTraining Loss: 1.597905\n",
      "Epoch: 22 \tTraining Loss: 1.596857\n",
      "Epoch: 23 \tTraining Loss: 1.595837\n",
      "Epoch: 24 \tTraining Loss: 1.594825\n",
      "Epoch: 25 \tTraining Loss: 1.587593\n",
      "Epoch: 26 \tTraining Loss: 1.545370\n",
      "Epoch: 27 \tTraining Loss: 1.535258\n",
      "Epoch: 28 \tTraining Loss: 1.530003\n",
      "Epoch: 29 \tTraining Loss: 1.526226\n",
      "Epoch: 30 \tTraining Loss: 1.523195\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    torch.save(model.state_dict(), 'mnist.pth')\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/torch-1.7-py3/lib/python3.7/site-packages/ipykernel/__main__.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.525036\n",
      "\n",
      "Test Accuracy of     0: 98% (967/980)\n",
      "Test Accuracy of     1: 97% (1111/1135)\n",
      "Test Accuracy of     2: 93% (968/1032)\n",
      "Test Accuracy of     3: 93% (940/1010)\n",
      "Test Accuracy of     4: 94% (930/982)\n",
      "Test Accuracy of     5: 87% (780/892)\n",
      "Test Accuracy of     6: 96% (920/958)\n",
      "Test Accuracy of     7: 95% (977/1028)\n",
      "Test Accuracy of     8: 90% (881/974)\n",
      "Test Accuracy of     9: 92% (936/1009)\n",
      "\n",
      "Test Accuracy (Overall): 94% (9410/10000)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layers.0.weight', tensor([[-0.0257,  0.0153, -0.0149,  ...,  0.0121, -0.0327, -0.0113],\n",
      "        [ 0.0036,  0.0118,  0.0057,  ..., -0.0330,  0.0328, -0.0177],\n",
      "        [-0.0334,  0.0222,  0.0084,  ...,  0.0267,  0.0290,  0.0150],\n",
      "        ...,\n",
      "        [-0.0264,  0.0132, -0.0152,  ...,  0.0126,  0.0142,  0.0229],\n",
      "        [ 0.0287,  0.0194,  0.0126,  ...,  0.0167,  0.0206,  0.0134],\n",
      "        [ 0.0302, -0.0192, -0.0185,  ...,  0.0163,  0.0287, -0.0252]])), ('layers.0.bias', tensor([-1.3767e-02, -1.7466e-02, -9.4647e-03,  5.9409e-03,  1.6467e-02,\n",
      "         3.9640e-02, -3.3521e-02, -1.7707e-02,  3.8793e-02,  3.3052e-02,\n",
      "         4.6234e-02,  4.9658e-02,  1.1988e-02, -6.2132e-03,  3.6456e-02,\n",
      "         3.1284e-02,  2.3535e-02,  5.0652e-03,  3.1584e-03, -1.0833e-02,\n",
      "        -1.4996e-02, -2.8061e-02, -5.4911e-02,  5.6590e-02,  7.8679e-04,\n",
      "        -5.3288e-03,  5.4412e-03, -2.9851e-02,  1.1851e-02, -1.4605e-02,\n",
      "         5.0437e-02,  1.0080e-02,  6.1243e-02, -1.3783e-02,  3.2884e-02,\n",
      "        -1.4498e-02, -2.8119e-02,  2.0592e-02,  1.0916e-02,  3.9798e-02,\n",
      "         5.2033e-02,  2.6539e-02, -1.3994e-02,  7.2427e-03, -2.3335e-02,\n",
      "         3.9209e-03, -1.6115e-02, -4.0121e-02, -8.9369e-03,  3.9834e-02,\n",
      "        -3.0163e-02, -1.1651e-02,  6.2176e-02,  4.7170e-02, -1.6613e-02,\n",
      "        -2.1295e-02,  3.6516e-02, -8.6727e-03,  1.3931e-02,  6.0339e-02,\n",
      "        -7.3828e-03, -1.3062e-03,  2.0241e-02, -1.1911e-02, -3.6337e-02,\n",
      "        -2.1577e-02,  3.9172e-02,  3.2571e-02,  1.0857e-02,  1.0412e-02,\n",
      "        -1.3204e-02, -7.6952e-03,  2.9656e-03,  7.9318e-02, -1.5104e-02,\n",
      "         7.0467e-02,  4.8128e-02,  1.0434e-02,  5.6395e-03,  5.6560e-02,\n",
      "         1.9161e-02, -3.1440e-03, -8.6395e-03,  2.5946e-02,  2.4390e-02,\n",
      "         5.9954e-02,  1.8793e-02,  4.6386e-02,  3.5173e-02,  3.7300e-02,\n",
      "         1.9564e-02,  1.0768e-02,  1.3095e-02, -2.4981e-02, -1.5581e-02,\n",
      "         4.2468e-03,  3.9692e-02,  8.1771e-03,  2.1258e-02,  5.0166e-02,\n",
      "         2.8425e-02,  6.7461e-03, -2.1696e-02,  5.1225e-02, -7.4353e-03,\n",
      "        -2.1393e-02, -2.3975e-02, -2.5720e-02, -8.8620e-04,  1.4934e-02,\n",
      "         1.2622e-02,  4.6544e-02,  7.4168e-02,  2.5365e-02,  2.9922e-02,\n",
      "        -8.3600e-03,  5.1602e-02, -3.2329e-02, -3.1405e-02,  3.8685e-02,\n",
      "        -1.4978e-02,  6.6631e-04,  3.0003e-02,  2.8875e-02,  5.4990e-02,\n",
      "         4.9650e-02,  3.3327e-02,  3.2647e-02,  3.6012e-02, -1.8064e-02,\n",
      "         5.0574e-02,  3.7934e-02,  1.7231e-02,  2.6865e-02, -2.8727e-02,\n",
      "        -2.1689e-02, -1.1325e-02,  6.7044e-02,  1.9082e-02, -1.2932e-02,\n",
      "         3.7244e-02,  2.6593e-02,  7.3997e-03,  1.7392e-02,  1.1645e-02,\n",
      "        -2.3322e-04, -1.1236e-02,  1.9925e-02, -2.5006e-02,  1.3084e-02,\n",
      "        -1.8960e-02, -6.6883e-02,  4.0660e-02, -1.9091e-02,  2.3494e-02,\n",
      "         3.6130e-02, -6.3702e-03,  1.5705e-02,  7.5185e-03,  1.4698e-02,\n",
      "         5.3252e-02, -5.2308e-03,  5.8250e-04, -2.0533e-02, -5.5914e-04,\n",
      "        -1.1570e-02,  1.0407e-02,  1.7899e-02,  1.9570e-02, -1.4144e-02,\n",
      "        -1.6838e-02, -6.5875e-03,  3.4646e-02,  5.0718e-03,  4.1559e-03,\n",
      "         5.9114e-04,  1.6340e-02,  3.8721e-02, -7.4761e-03,  1.0735e-02,\n",
      "        -2.3744e-02,  3.8664e-02, -6.0694e-03, -1.6133e-02,  1.9829e-02,\n",
      "        -1.7378e-02,  3.2097e-02, -1.8648e-02, -2.6086e-02, -3.5936e-02,\n",
      "         2.4278e-02, -4.4797e-03,  2.5076e-02,  3.9113e-02, -2.9293e-02,\n",
      "         2.1670e-02,  1.2899e-02, -8.4057e-03,  7.4104e-03,  1.3644e-02,\n",
      "         7.7698e-03, -1.0522e-02, -1.3166e-02, -5.4895e-02,  3.2549e-02,\n",
      "        -2.0855e-03,  4.9513e-02, -1.9995e-02, -2.9380e-02,  3.8967e-02,\n",
      "         5.4309e-02, -2.3921e-03, -4.1973e-02,  2.6847e-03,  1.6190e-02,\n",
      "        -2.4742e-02, -3.7466e-02,  4.3283e-02, -2.7386e-03, -1.1389e-02,\n",
      "         9.5452e-03,  1.4836e-02, -1.8464e-02, -1.0503e-02,  6.8817e-02,\n",
      "         2.1207e-02, -1.3709e-02, -5.7380e-03,  3.3096e-02,  7.6703e-03,\n",
      "         2.8174e-02, -2.2471e-02, -3.6225e-02, -3.1809e-02,  2.5023e-02,\n",
      "        -1.1754e-02,  3.9325e-02,  1.3918e-02,  9.8805e-03, -7.3454e-02,\n",
      "        -2.9918e-02,  1.0080e-02,  8.4095e-02,  3.7796e-02, -1.3716e-03,\n",
      "        -2.7845e-02,  2.0033e-02, -8.6453e-03,  1.3974e-02,  4.4138e-02,\n",
      "        -3.1357e-02, -9.5499e-03,  7.4040e-03,  4.5190e-02, -2.1499e-02,\n",
      "         9.8604e-03, -1.1746e-03,  4.7372e-03,  3.6329e-02,  9.1691e-03,\n",
      "         3.0240e-02,  2.8383e-02,  5.0831e-02, -3.2629e-03,  5.0502e-02,\n",
      "        -3.1075e-02,  3.2100e-02, -4.3849e-03,  2.0507e-02, -1.6692e-02,\n",
      "         5.8276e-02, -1.5511e-03,  4.0011e-03, -3.3418e-03, -1.2347e-02,\n",
      "         2.0616e-02,  1.9379e-02,  9.2751e-03,  1.6462e-02, -1.2155e-02,\n",
      "         5.4806e-03, -2.0178e-02,  3.0268e-02,  6.3009e-03, -2.3506e-02,\n",
      "         8.5931e-03,  6.5155e-02,  7.4103e-02, -8.4230e-03, -1.2838e-02,\n",
      "        -2.8993e-02, -3.3414e-03, -2.5151e-02, -8.5025e-03,  2.6393e-02,\n",
      "         2.3101e-03, -1.0727e-03, -1.2486e-03,  2.6848e-02, -1.3650e-03,\n",
      "        -2.6831e-02,  4.0814e-02, -3.6062e-02, -7.4312e-04,  1.0214e-02,\n",
      "         4.8505e-02,  8.7123e-03, -7.0796e-04,  2.3124e-02, -1.7475e-02,\n",
      "        -1.5690e-02, -3.1476e-02, -2.9983e-02, -3.2804e-02, -1.6348e-02,\n",
      "         8.2672e-03, -1.9546e-02,  2.0987e-02,  1.4932e-02,  4.4679e-02,\n",
      "        -4.3483e-04,  3.2500e-02, -2.2325e-02,  3.0558e-03,  2.2622e-02,\n",
      "        -2.9682e-02, -6.2027e-03,  2.1712e-02,  5.2967e-02, -1.2436e-02,\n",
      "         1.9708e-03,  3.5528e-02,  3.8707e-02, -1.3325e-02, -6.8578e-03,\n",
      "         5.0442e-05, -4.1969e-03,  4.4152e-02,  3.8546e-03,  1.1175e-02,\n",
      "         5.6441e-02,  8.3853e-03, -1.8381e-02,  1.0661e-01,  7.5597e-03,\n",
      "         9.5291e-03, -9.1105e-03, -3.2675e-04,  3.7312e-02,  6.6624e-03,\n",
      "         1.6092e-02, -2.4642e-03,  6.7503e-02,  4.6493e-02,  8.7777e-03,\n",
      "        -2.7358e-02, -4.4771e-02, -2.9452e-02,  4.4027e-02, -2.0545e-02,\n",
      "        -2.2154e-02, -2.4419e-02,  1.8397e-02,  4.1282e-02, -4.9043e-02,\n",
      "         4.6609e-02,  6.9667e-02, -7.2837e-03,  5.6085e-02, -1.5465e-03,\n",
      "         3.2346e-02, -1.3075e-02, -6.9674e-03,  3.2137e-03,  1.6039e-02,\n",
      "        -1.4819e-02,  1.6080e-02,  1.9870e-02,  4.5662e-02,  5.0301e-02,\n",
      "         5.6822e-02,  6.1338e-02,  5.5989e-02,  6.0861e-02,  3.0694e-02,\n",
      "         3.7918e-03, -1.2381e-02,  2.7788e-03, -2.6943e-02, -1.1655e-02,\n",
      "         2.4746e-02,  2.8198e-02, -7.3730e-02, -2.8647e-02,  1.9693e-02,\n",
      "        -1.7544e-02, -3.0929e-02,  6.2220e-02,  4.7148e-02,  2.1206e-02,\n",
      "         3.8284e-02,  2.5898e-02,  5.8693e-02,  6.4300e-02,  1.5127e-03,\n",
      "         5.0924e-02, -4.7814e-02, -2.7529e-02, -2.5711e-02,  1.1829e-02,\n",
      "         2.3525e-02,  1.2305e-02, -2.0332e-02, -2.8637e-02,  4.8996e-02,\n",
      "         2.5064e-03,  3.7536e-02,  2.8932e-02,  5.7777e-02,  2.8767e-02,\n",
      "        -9.7901e-03,  8.9213e-03,  1.0578e-02, -3.5417e-02,  1.9737e-02,\n",
      "         5.1377e-02,  1.7690e-02,  2.5437e-02,  2.3744e-02,  4.5993e-02,\n",
      "        -3.9833e-02, -9.3515e-03,  1.0423e-02,  7.1875e-02,  1.0692e-02,\n",
      "         2.8739e-02,  3.9801e-02,  1.1890e-02,  4.5629e-02,  4.7379e-02,\n",
      "         9.7893e-03,  7.4162e-03, -2.6244e-02,  3.4893e-03, -3.4386e-02,\n",
      "         2.3702e-02,  3.9669e-03, -1.5573e-02, -4.1434e-02, -6.8167e-04,\n",
      "        -3.2499e-02,  4.5268e-02,  3.1903e-02,  3.6428e-02,  3.7593e-02,\n",
      "         5.5431e-03, -4.3390e-03,  1.0582e-02,  2.3072e-02,  2.6105e-02,\n",
      "        -2.1692e-02,  2.2347e-02,  4.8079e-02, -1.8099e-02,  2.7801e-02,\n",
      "         1.1949e-02, -5.2841e-02,  1.9476e-02,  4.5458e-02,  4.8374e-02,\n",
      "         3.4529e-02,  8.8661e-02,  2.3327e-02, -1.4063e-02, -1.8267e-02,\n",
      "        -1.4948e-02,  7.9984e-03, -2.6876e-02,  1.1218e-04, -7.2602e-03,\n",
      "         2.9467e-02,  1.7095e-02, -8.7752e-03,  2.1407e-02,  2.3301e-02,\n",
      "         4.0608e-02,  1.6577e-02,  1.2965e-02,  2.5766e-02,  9.0254e-03,\n",
      "         3.8208e-02, -1.3017e-02, -2.4932e-02,  2.3895e-02,  1.2376e-02,\n",
      "         3.7177e-02, -5.7588e-03,  6.8710e-02,  1.6896e-02,  8.5601e-02,\n",
      "         1.2901e-02,  1.0306e-02,  6.3881e-03,  1.2274e-02,  3.6695e-02,\n",
      "        -6.1324e-03,  4.7718e-02, -2.1428e-02, -9.3137e-03, -3.0441e-02,\n",
      "         5.1082e-02,  3.2814e-02])), ('layers.2.weight', tensor([[-0.0546,  0.0027, -0.0454,  ..., -0.0062,  0.0157, -0.0036],\n",
      "        [ 0.0280,  0.0431,  0.0340,  ..., -0.0376,  0.0205,  0.0203],\n",
      "        [-0.0187,  0.0230,  0.0074,  ...,  0.0432, -0.0148, -0.0083],\n",
      "        ...,\n",
      "        [-0.0298, -0.0277,  0.0243,  ..., -0.0172, -0.0503,  0.0344],\n",
      "        [-0.0388, -0.0101, -0.0075,  ..., -0.0371,  0.0396, -0.0528],\n",
      "        [ 0.0091, -0.0148, -0.0276,  ...,  0.0252, -0.0079, -0.0145]])), ('layers.2.bias', tensor([-1.2672e-03,  1.7167e-02,  3.0353e-02,  2.8344e-02, -4.7652e-02,\n",
      "         3.6323e-02, -1.2457e-02,  4.0171e-02, -7.7309e-03, -5.3051e-02,\n",
      "        -1.3928e-02,  2.3438e-02, -1.8211e-02, -2.5741e-02, -1.0346e-02,\n",
      "         1.1177e-02,  1.0646e-03,  9.8296e-04, -4.8808e-03, -1.4642e-02,\n",
      "         2.7551e-02, -2.4252e-02, -6.7994e-03, -4.8489e-03, -1.8121e-02,\n",
      "         1.4865e-02,  5.7595e-03,  5.2908e-02, -4.2500e-02, -2.2889e-02,\n",
      "         9.9889e-03, -9.6668e-03,  6.7092e-04,  2.0874e-02, -5.0695e-04,\n",
      "        -3.4557e-02, -4.9671e-02,  4.5026e-02, -2.8748e-02,  1.2752e-02,\n",
      "         5.6346e-02,  4.1997e-02, -6.5372e-02, -2.3033e-02, -2.1234e-02,\n",
      "         2.0577e-02, -9.8292e-03, -2.0584e-02,  1.2357e-02,  2.6090e-02,\n",
      "        -8.9333e-03, -1.4740e-02,  1.0308e-02, -2.0880e-02,  5.4169e-04,\n",
      "        -2.2906e-03,  3.6676e-02, -1.4858e-02,  2.4405e-02, -3.1959e-02,\n",
      "        -1.7845e-02, -2.5313e-02, -1.2175e-02, -1.8275e-02, -2.6441e-02,\n",
      "        -2.5635e-02, -4.2924e-02, -1.4776e-02,  6.2669e-02,  2.4722e-02,\n",
      "         7.5995e-02,  5.0280e-02,  4.8908e-02, -1.0749e-02,  3.1304e-02,\n",
      "         4.2493e-02,  6.2515e-02, -3.5571e-02,  1.3953e-02, -4.3692e-02,\n",
      "         5.3002e-03, -4.6304e-03, -3.8554e-02,  6.3413e-02, -1.8250e-02,\n",
      "         3.9804e-02, -2.6359e-02,  6.6940e-03, -4.2636e-02, -2.1105e-02,\n",
      "         3.6899e-02, -6.7517e-03,  3.3343e-02, -2.3899e-02, -7.4662e-03,\n",
      "         6.1800e-03,  1.9653e-02,  3.5350e-02,  8.7539e-03,  4.4756e-02,\n",
      "         6.8083e-02, -1.1593e-02,  9.0035e-03,  1.1299e-02, -8.7905e-03,\n",
      "         3.8747e-02,  2.4063e-02,  3.1275e-02,  2.4894e-02,  7.9013e-02,\n",
      "        -3.1119e-02, -4.1944e-02,  4.3109e-02,  7.8405e-02,  2.2870e-02,\n",
      "        -1.5615e-02, -3.3186e-03, -2.9272e-02,  3.7337e-02,  3.4550e-02,\n",
      "        -3.3801e-02,  7.7382e-04, -5.8298e-02,  4.4717e-02,  4.7968e-02,\n",
      "         5.4009e-02,  2.9832e-03,  7.3241e-04, -4.0881e-02,  1.7461e-02,\n",
      "         3.3800e-02,  4.9411e-02,  4.3598e-02,  1.1244e-03, -4.3282e-02,\n",
      "         2.1317e-03,  4.4602e-02,  4.1945e-02,  1.9531e-02,  1.4831e-02,\n",
      "         2.9204e-02, -2.7429e-02, -3.9849e-03, -1.9861e-02,  2.9407e-02,\n",
      "         5.0841e-02,  6.2674e-03, -2.1619e-02,  3.9032e-02, -1.2287e-02,\n",
      "         2.6298e-02,  1.5017e-02,  8.9579e-04,  5.1299e-03,  2.2531e-02,\n",
      "         1.4668e-02, -1.1163e-02,  6.2963e-02,  2.2039e-02,  3.1160e-02,\n",
      "        -8.8233e-03,  1.8987e-02,  2.1753e-02, -3.6606e-02, -2.1373e-02,\n",
      "        -9.5842e-03, -2.6859e-02,  3.8847e-02,  1.5048e-02, -1.6130e-02,\n",
      "        -4.1675e-03, -9.7011e-03,  8.7313e-04, -2.2323e-02, -1.3271e-02,\n",
      "        -1.5828e-02,  9.3569e-02,  1.6682e-02, -1.9545e-02,  2.2588e-03,\n",
      "        -3.7713e-03, -1.9283e-02,  1.5820e-03,  2.6246e-02,  5.0666e-03,\n",
      "         2.6039e-02,  1.5001e-02,  2.3265e-03, -2.9075e-02,  2.3666e-02,\n",
      "         1.7130e-02,  1.4534e-02, -2.4443e-02,  6.5461e-02,  3.6366e-03,\n",
      "         3.6593e-02, -2.1552e-02,  2.9180e-02,  2.3833e-03, -4.5837e-02,\n",
      "         1.3609e-02,  3.0252e-02,  2.2417e-02, -6.1549e-03,  5.4810e-02,\n",
      "         4.2906e-02,  9.7274e-03, -3.5813e-02,  3.4836e-03,  1.3198e-02,\n",
      "        -2.3948e-02,  5.5630e-02, -1.3399e-02,  1.1569e-02,  2.6484e-02,\n",
      "         1.5541e-02,  1.0949e-02, -6.4250e-02,  2.0139e-02, -1.0258e-02,\n",
      "        -9.2050e-03, -4.2424e-02,  1.8123e-02, -3.4066e-02,  9.7768e-03,\n",
      "        -2.7765e-02,  6.4832e-02, -3.7867e-02,  2.2036e-02,  6.3849e-02,\n",
      "         4.3903e-03,  2.9871e-02, -2.7000e-02,  4.4872e-02, -3.7617e-02,\n",
      "        -3.2313e-02, -1.0842e-02,  1.5264e-02, -5.9650e-04, -1.0731e-02,\n",
      "         2.7363e-02,  4.1349e-02,  3.4675e-02,  4.9952e-02, -4.9942e-03,\n",
      "         1.6801e-02,  4.5467e-02,  1.6117e-02,  4.5839e-03,  4.1662e-02,\n",
      "         4.8925e-03,  9.6182e-03, -2.7352e-02, -1.4394e-02, -3.6266e-02,\n",
      "         2.3284e-02, -2.5187e-02,  2.5704e-02, -5.1749e-02, -1.7541e-02,\n",
      "        -4.0176e-02,  2.6474e-03, -6.1093e-03, -1.9241e-02,  1.0020e-02,\n",
      "         2.2571e-02,  1.9414e-02,  9.1649e-02, -1.6303e-02, -1.5765e-02,\n",
      "        -3.5825e-02, -3.8981e-02,  1.3250e-02, -7.3729e-03,  2.7902e-02,\n",
      "         7.9512e-03,  4.9290e-02,  2.0929e-02,  6.6584e-03,  1.2303e-02,\n",
      "        -1.1568e-02,  2.7779e-02,  7.8046e-03, -2.9994e-02, -1.9515e-02,\n",
      "         5.5515e-02,  6.1432e-02,  3.6761e-02,  4.3690e-02, -3.1080e-02,\n",
      "         2.1032e-02, -1.3794e-02, -3.6830e-02, -3.8122e-02,  3.9546e-02,\n",
      "         4.9321e-03, -2.4016e-02,  3.0391e-02,  3.1635e-02, -2.3173e-02,\n",
      "         2.3938e-03,  1.0234e-02, -2.0377e-02,  3.5547e-02,  3.7417e-02,\n",
      "         1.2518e-03,  8.0574e-02, -7.9019e-03, -4.1861e-02,  1.4094e-02,\n",
      "         1.6836e-02, -2.0945e-02,  1.2179e-02,  2.0027e-02, -2.6499e-02,\n",
      "        -7.1394e-03,  4.9492e-02,  1.9638e-02, -8.8054e-03, -1.4676e-02,\n",
      "        -8.1717e-03,  7.1458e-02,  6.1715e-02, -3.3492e-02, -1.6023e-02,\n",
      "        -9.7466e-03,  1.0649e-02,  2.2004e-02, -1.7576e-02,  3.3164e-02,\n",
      "         1.8766e-02, -2.0083e-02, -1.5010e-02, -1.8005e-03, -2.5823e-02,\n",
      "         3.4786e-02,  5.9975e-02, -2.7745e-02,  4.5126e-02,  3.3987e-03,\n",
      "         4.0329e-02,  2.9125e-04,  1.0929e-02,  4.2139e-02, -3.1994e-02,\n",
      "         5.1192e-02,  4.5409e-02, -2.4869e-02,  4.1354e-02,  1.7666e-02,\n",
      "         2.3883e-02, -2.9027e-02, -1.7565e-02,  3.4557e-02,  2.2066e-02,\n",
      "        -4.4305e-02, -2.7289e-02,  1.1442e-02, -3.1454e-02,  1.2960e-02,\n",
      "        -3.5015e-02,  3.6367e-02,  4.3360e-02,  4.7715e-02, -1.9945e-02,\n",
      "         2.5772e-02, -1.3407e-03,  2.3971e-02,  7.3276e-02, -1.5482e-02,\n",
      "         5.0102e-02, -1.2463e-02,  4.5324e-02,  1.3819e-02,  1.8992e-02,\n",
      "         2.7901e-02,  6.9986e-04,  4.5886e-02,  7.5176e-02,  3.8313e-02,\n",
      "         3.2334e-02,  6.8154e-02,  2.6340e-02,  5.1073e-02, -7.2692e-03,\n",
      "         3.2358e-02, -1.0685e-02,  4.0647e-02,  7.0127e-03, -3.5149e-02,\n",
      "        -7.2339e-02,  5.4218e-05,  4.6393e-02, -2.2779e-02, -2.4017e-02,\n",
      "         1.9991e-02, -9.6508e-03,  5.0020e-02,  3.2793e-02,  1.9083e-02,\n",
      "         4.7058e-02, -3.6889e-02, -5.4439e-02,  1.7446e-02,  7.6261e-02,\n",
      "        -1.6534e-03,  9.9004e-03,  4.4176e-02, -1.0841e-02, -2.0310e-02,\n",
      "        -7.5440e-03, -1.6710e-02,  1.0416e-02, -2.5237e-02,  5.9780e-02,\n",
      "        -4.6415e-04,  2.6909e-02, -2.7723e-02,  6.8752e-02,  3.6786e-02,\n",
      "         7.9834e-02,  8.8296e-03, -3.4085e-03,  1.9735e-02,  3.6696e-02,\n",
      "        -2.5034e-02,  1.9319e-02,  8.4827e-03,  3.4727e-02,  6.8757e-02,\n",
      "        -2.1947e-03,  9.6249e-03,  1.9648e-02,  1.9254e-02,  2.7832e-03,\n",
      "         3.3078e-02,  5.4396e-02,  1.9492e-03,  3.6912e-03, -1.6846e-02,\n",
      "         2.7622e-02, -2.6871e-03,  3.4846e-02,  5.0498e-02, -3.3807e-02,\n",
      "        -2.3292e-02,  4.3298e-02,  1.6975e-02, -3.1332e-02, -4.2272e-02,\n",
      "         3.1012e-02,  2.5013e-02, -4.7232e-03, -1.4187e-02, -4.4886e-03,\n",
      "        -1.6632e-03,  4.1114e-02,  3.4773e-02,  2.5307e-02,  5.1298e-02,\n",
      "         2.0909e-02, -2.3911e-02,  5.7357e-02,  5.6024e-02, -2.7274e-02,\n",
      "        -6.1513e-03,  3.1493e-02, -3.7573e-02,  3.3176e-02,  8.3482e-03,\n",
      "         2.8161e-02, -4.2577e-02,  1.7857e-02, -5.5499e-02,  1.3279e-02,\n",
      "         5.5747e-02,  3.3840e-02,  6.2389e-02, -2.7559e-02,  5.5685e-02,\n",
      "        -2.5071e-02, -2.6334e-02, -6.8371e-03,  4.0658e-02,  6.4074e-02,\n",
      "         3.2014e-02,  4.3381e-02,  4.3348e-02, -2.2496e-02,  3.4178e-02,\n",
      "         5.2604e-02,  2.0052e-02,  7.2114e-02,  5.0030e-02, -4.3966e-02,\n",
      "        -4.2149e-02,  1.0351e-02,  3.6127e-02,  2.3778e-02,  2.5656e-02,\n",
      "         3.0554e-02,  3.9631e-02,  9.3298e-03,  1.7655e-02,  6.0894e-02,\n",
      "        -1.6937e-02,  1.0426e-02, -3.1295e-02, -1.8251e-02,  3.3649e-02,\n",
      "         2.9394e-02, -2.2487e-03])), ('layers.4.weight', tensor([[-0.0635,  0.0278, -0.0126,  ...,  0.2100,  0.0763, -0.1303],\n",
      "        [ 0.0258, -0.0880,  0.0652,  ..., -0.0483, -0.0923, -0.2505],\n",
      "        [ 0.0377, -0.1108,  0.0685,  ..., -0.0247, -0.0452, -0.2477],\n",
      "        ...,\n",
      "        [ 0.1891,  0.1159,  0.1158,  ...,  0.0305, -0.0203, -0.0314],\n",
      "        [-0.0505,  0.0371, -0.0725,  ...,  0.1120, -0.1363,  0.0723],\n",
      "        [-0.1786,  0.1738, -0.0883,  ...,  0.0958, -0.1583,  0.2260]])), ('layers.4.bias', tensor([-0.1601,  0.2133, -0.0487, -0.0266,  0.0893,  0.0894,  0.0115,  0.0486,\n",
      "        -0.1898, -0.0496]))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "base_model_state_dict = torch.load(\"mnist.pth\")\n",
    "print(base_model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784)\n",
      "(512, 1)\n",
      "(512, 785) (512, 513) (10, 513)\n",
      "(669706, 1)\n"
     ]
    }
   ],
   "source": [
    "print(base_model_state_dict['layers.0.weight'].cpu().numpy().shape)\n",
    "print(base_model_state_dict['layers.0.bias'].cpu().numpy().reshape(-1,1).shape)\n",
    "a = np.hstack((base_model_state_dict['layers.0.weight'].cpu().numpy(),base_model_state_dict['layers.0.bias'].cpu().numpy().reshape(-1,1)))\n",
    "b = np.hstack((base_model_state_dict['layers.2.weight'].cpu().numpy(),base_model_state_dict['layers.2.bias'].cpu().numpy().reshape(-1,1)))\n",
    "c = np.hstack((base_model_state_dict['layers.4.weight'].cpu().numpy(),base_model_state_dict['layers.4.bias'].cpu().numpy().reshape(-1,1)))\n",
    "print(a.shape,b.shape,c.shape)\n",
    "a = np.vstack((a.reshape(-1,1),b.reshape(-1,1)))\n",
    "a = np.vstack((a.reshape(-1,1),c.reshape(-1,1)))\n",
    "\n",
    "\n",
    "a = a.reshape(-1,1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[238  32 218 130  48  49  64 191 197 243  11 139 142 238 138  45 231 130\n",
      " 163  11 175 139  77 194 249  96  89 123  77  32]\n",
      "[[ 3.54634784e-02]\n",
      " [-1.67179406e-02]\n",
      " [ 3.45787103e-03]\n",
      " [-3.23950499e-02]\n",
      " [ 1.76184736e-02]\n",
      " [ 6.63152635e-02]\n",
      " [-1.36610374e-01]\n",
      " [-4.40531038e-02]\n",
      " [ 1.98211119e-01]\n",
      " [-1.50254369e-03]\n",
      " [ 5.43002449e-02]\n",
      " [ 3.05306874e-02]\n",
      " [-1.05894953e-02]\n",
      " [-2.38550100e-02]\n",
      " [ 1.11204451e-02]\n",
      " [-7.87975416e-02]\n",
      " [ 1.10937171e-01]\n",
      " [ 2.56222282e-02]\n",
      " [-2.17148051e-01]\n",
      " [-5.52386381e-02]\n",
      " [ 4.81988601e-02]\n",
      " [ 3.04292113e-01]\n",
      " [ 4.26697358e-02]\n",
      " [-6.71587978e-03]\n",
      " [ 1.52097419e-01]\n",
      " [ 7.75616895e-03]\n",
      " [-2.08740532e-02]\n",
      " [-3.92614529e-02]\n",
      " [-2.64439508e-02]\n",
      " [-1.07876435e-01]\n",
      " [ 2.21790802e-02]\n",
      " [ 7.39600211e-02]\n",
      " [ 1.53630590e-02]\n",
      " [-6.47041351e-02]\n",
      " [-1.30365174e-02]\n",
      " [-3.09851598e-02]\n",
      " [-2.51414955e-01]\n",
      " [ 3.88773009e-02]\n",
      " [ 8.63359943e-02]\n",
      " [-3.61812413e-02]\n",
      " [-1.73028722e-01]\n",
      " [-3.84042901e-03]\n",
      " [ 3.29592489e-02]\n",
      " [ 2.44655356e-01]\n",
      " [-4.75251153e-02]\n",
      " [ 3.07169510e-04]\n",
      " [ 5.85242920e-02]\n",
      " [-2.76371762e-02]\n",
      " [ 5.31128235e-03]\n",
      " [ 2.81429086e-02]\n",
      " [-3.58734339e-01]\n",
      " [ 1.35223344e-01]\n",
      " [-1.95542425e-02]\n",
      " [ 1.37413237e-02]\n",
      " [-7.83262122e-03]\n",
      " [ 2.01382022e-02]\n",
      " [ 2.42593288e-02]\n",
      " [ 9.47891083e-03]\n",
      " [-9.37471166e-02]\n",
      " [-4.96667214e-02]\n",
      " [-4.09004949e-02]\n",
      " [-1.23084471e-01]\n",
      " [-1.52615365e-02]\n",
      " [ 9.75719243e-02]\n",
      " [-3.37226875e-02]\n",
      " [-6.24355935e-02]\n",
      " [ 6.16176613e-02]\n",
      " [-1.52645528e-01]\n",
      " [-2.21636519e-02]\n",
      " [ 4.11170162e-02]\n",
      " [ 1.79108858e-01]\n",
      " [-4.99707833e-03]\n",
      " [ 5.19529395e-02]\n",
      " [ 4.57442217e-02]\n",
      " [ 3.67809087e-02]\n",
      " [-1.90020710e-01]\n",
      " [ 2.76605338e-01]\n",
      " [ 1.57555786e-03]\n",
      " [-2.89428011e-02]\n",
      " [ 3.32073361e-01]\n",
      " [ 2.06134602e-01]\n",
      " [-3.76360081e-02]\n",
      " [-9.45284404e-03]\n",
      " [ 6.52985182e-03]\n",
      " [ 1.88467018e-02]\n",
      " [-1.17969643e-02]\n",
      " [ 7.87015781e-02]\n",
      " [-3.14147115e-01]\n",
      " [-1.83013510e-02]\n",
      " [ 2.35586483e-02]\n",
      " [-5.23135066e-02]\n",
      " [-5.85746095e-02]\n",
      " [ 2.93438919e-02]\n",
      " [ 2.26259992e-01]\n",
      " [ 2.69197710e-02]\n",
      " [-7.28115216e-02]\n",
      " [ 1.27027361e-02]\n",
      " [-4.57267128e-02]\n",
      " [ 3.17433141e-02]\n",
      " [-1.42087238e-02]\n",
      " [ 2.14998461e-02]\n",
      " [-2.64903903e-01]\n",
      " [-2.48666760e-02]\n",
      " [ 1.63946331e-01]\n",
      " [ 3.76969576e-01]\n",
      " [ 3.42043750e-02]\n",
      " [ 1.41125605e-01]\n",
      " [ 1.23060383e-01]\n",
      " [-2.67729140e-03]\n",
      " [ 1.42748784e-02]\n",
      " [ 1.64757818e-02]\n",
      " [-2.96161752e-02]\n",
      " [-8.57746825e-02]\n",
      " [-2.06664085e-01]\n",
      " [ 4.03554477e-02]\n",
      " [-1.66561976e-01]\n",
      " [ 2.83344183e-03]\n",
      " [ 4.99547087e-02]\n",
      " [ 4.07594163e-03]\n",
      " [ 7.19254240e-02]\n",
      " [ 5.70370406e-02]\n",
      " [ 2.53924489e-01]\n",
      " [ 1.05842631e-02]\n",
      " [-3.49398032e-02]\n",
      " [-4.17214558e-02]\n",
      " [ 4.34327982e-02]\n",
      " [-3.05730035e-04]\n",
      " [ 7.14911986e-03]\n",
      " [-1.24217244e-02]\n",
      " [-1.32741168e-01]\n",
      " [ 8.34935345e-03]\n",
      " [ 4.18957137e-02]\n",
      " [-1.13378681e-01]\n",
      " [ 1.01753168e-01]\n",
      " [-6.15147967e-03]\n",
      " [ 1.16436714e-02]\n",
      " [ 6.99600354e-02]\n",
      " [-3.16880718e-02]\n",
      " [-1.89078841e-02]\n",
      " [-2.27654111e-02]\n",
      " [ 6.47368878e-02]\n",
      " [-2.29510069e-01]\n",
      " [-2.02134512e-02]\n",
      " [ 5.91494981e-03]\n",
      " [-8.91651493e-03]\n",
      " [ 4.49690931e-02]\n",
      " [ 7.62943104e-02]\n",
      " [ 6.01087771e-02]\n",
      " [-3.30764391e-02]\n",
      " [ 1.94790009e-02]\n",
      " [ 2.20151013e-03]\n",
      " [-1.02813914e-01]\n",
      " [-3.84396277e-02]\n",
      " [ 5.30760325e-02]\n",
      " [-1.81333989e-01]\n",
      " [ 2.08144262e-02]\n",
      " [ 2.49483306e-02]\n",
      " [-6.72052205e-02]\n",
      " [ 3.81568037e-02]\n",
      " [ 2.28543598e-02]\n",
      " [-9.13653290e-04]\n",
      " [-5.09293936e-02]\n",
      " [-2.82792300e-02]\n",
      " [-4.41619381e-03]\n",
      " [-1.46964595e-01]\n",
      " [-6.04656227e-02]\n",
      " [-1.62366405e-02]\n",
      " [-4.32849787e-02]\n",
      " [-2.70171613e-02]\n",
      " [ 2.99297310e-02]\n",
      " [ 1.17108576e-01]\n",
      " [ 1.48113193e-02]\n",
      " [ 3.48281637e-02]\n",
      " [ 9.36282799e-02]\n",
      " [-6.98889941e-02]\n",
      " [-3.02929152e-02]\n",
      " [ 9.40153957e-04]\n",
      " [ 3.61214876e-02]\n",
      " [ 5.56137785e-02]\n",
      " [-2.43662652e-02]\n",
      " [ 2.87421234e-02]\n",
      " [ 4.65420783e-02]\n",
      " [ 3.14666778e-01]\n",
      " [ 3.11364122e-02]\n",
      " [-7.27785751e-03]\n",
      " [-3.68816182e-02]\n",
      " [ 1.71928465e-01]\n",
      " [-8.97212178e-02]\n",
      " [ 2.15550303e-01]\n",
      " [-5.57744317e-03]\n",
      " [ 1.90996990e-01]\n",
      " [ 4.69023921e-03]\n",
      " [-4.65980396e-02]\n",
      " [-2.83649981e-01]\n",
      " [ 1.70433316e-02]\n",
      " [-5.68683408e-02]\n",
      " [-1.59391046e-01]\n",
      " [-2.08716933e-03]\n",
      " [ 2.62830760e-02]\n",
      " [-7.57618770e-02]\n",
      " [ 3.74536291e-02]\n",
      " [ 4.73578721e-02]\n",
      " [ 3.23488116e-02]\n",
      " [ 8.36868882e-02]\n",
      " [-2.15308964e-02]\n",
      " [ 1.59144271e-02]\n",
      " [ 3.35825793e-02]\n",
      " [ 2.75391415e-02]\n",
      " [-5.37744835e-02]\n",
      " [-1.72014777e-02]\n",
      " [-9.81526822e-02]\n",
      " [-2.53672060e-02]\n",
      " [-1.36383735e-02]\n",
      " [ 8.94489363e-02]\n",
      " [-4.00777534e-02]\n",
      " [-8.23601186e-02]\n",
      " [ 1.06342115e-01]\n",
      " [ 6.31354079e-02]\n",
      " [-1.47460271e-02]\n",
      " [-3.34715962e-01]\n",
      " [ 1.32211829e-02]\n",
      " [ 1.82223432e-02]\n",
      " [ 1.28942013e-01]\n",
      " [ 3.96114402e-02]\n",
      " [-1.98735014e-01]\n",
      " [-3.25562130e-03]\n",
      " [-2.41062477e-01]\n",
      " [-2.33253464e-02]\n",
      " [-1.11881373e-02]\n",
      " [ 2.62859851e-01]\n",
      " [-1.41794562e-01]\n",
      " [-3.55329663e-02]\n",
      " [-8.37747846e-03]\n",
      " [-1.28003910e-01]\n",
      " [ 2.93657273e-01]\n",
      " [ 4.41861339e-02]\n",
      " [ 1.21719502e-02]\n",
      " [-3.43484543e-02]\n",
      " [-2.58926041e-02]\n",
      " [ 4.90628891e-02]\n",
      " [-4.85609584e-02]\n",
      " [ 6.80519864e-02]\n",
      " [-1.00096725e-02]\n",
      " [ 8.92296620e-03]\n",
      " [ 1.58311531e-01]\n",
      " [-1.77317783e-02]\n",
      " [ 8.13384503e-02]\n",
      " [ 2.33749852e-01]\n",
      " [ 1.85138017e-01]\n",
      " [ 1.00330422e-02]\n",
      " [-4.25150916e-02]\n",
      " [-4.48660068e-02]\n",
      " [ 1.46533057e-01]\n",
      " [ 5.09201474e-02]\n",
      " [-1.57553162e-02]\n",
      " [-1.18392184e-01]]\n",
      "0.03183060139417648\n"
     ]
    }
   ],
   "source": [
    "k_means = KMeans(n_clusters=256)\n",
    "\n",
    "k_means.fit(a)\n",
    "y_predict = k_means.predict(a)\n",
    "\n",
    "plt.scatter(a[:,0],a[:,0],c=y_predict)\n",
    "print(k_means.predict((a[:30,:])))\n",
    "print(k_means.cluster_centers_)\n",
    "np.save('256.npy',k_means.cluster_centers_)\n",
    "print(k_means.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch-1.7-py3]",
   "language": "python",
   "name": "conda-env-torch-1.7-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
