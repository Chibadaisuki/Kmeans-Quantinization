import os
import sys
import numpy as np
from PIL import Image
import pandas as pd

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

import torch.optim as optim
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import matplotlib.pyplot as plt
import time
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   # see issue #152
os.environ["CUDA_VISIBLE_DEVICES"]="4"
cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu")
num_workers = 4 if cuda else 0
train_transforms = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(p=0.5),
    torchvision.transforms.RandomRotation(degrees=25),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225]),
])
test_transforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225]),
])
## Dataloaders

# Training dataloader
train_dataset = torchvision.datasets.ImageFolder(root='/data4/wuyuchen/hw2p2s1-face-classification.zip_files/train_data', 
                                                 transform=train_transforms)
train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=48)
# Evaluating dataloader
eval_dataset = torchvision.datasets.ImageFolder(root='/data4/wuyuchen/hw2p2s1-face-classification.zip_files/val_data', 
                                                 transform=test_transforms)
eval_dataloader = DataLoader(eval_dataset, batch_size = 128, shuffle=True, num_workers=48)
# Train the Model
def train_model(train_loader, model):
  # Set model to training mode
  model.train()
  grad = {}
  training_loss = 0.0
  params = list(model.named_parameters())
  for i in range(105):
        (name, param) = params[i]
        grad[name] = torch.zeros(param.shape).to(device)
    

  # Enumerate through batches
  for batch_num, (inputs, target) in enumerate(train_loader):
    # Clear gradient in optimizer
    optimizer.zero_grad()

    # Mover inputs and target to device
    inputs = inputs.to(device)
    target = target.to(device)

    # Compute model output
    outputs = model(inputs)

    # Calculate Loss
    loss = criterion(outputs, target)
    loss.backward()
    optimizer.step()
    base_model_state_dict = model.state_dict()
    params = list(model.named_parameters())
    for i in range(105):
        (name, param) = params[i]
        grad[name]+=param.grad
    training_loss += loss.item()
  torch.save(grad, 'grad.pth')
  #training_loss /= len(train_dataset)
  return training_loss
  def evaluate_model(val_loader, model):
  # Set model to evaluating mode
  model.eval()

  num_correct = 0

  loss_value = 0
  with torch.no_grad():
    for batch_num, (inputs, target) in enumerate(val_loader):
      inputs = inputs.to(device)
      target = target.to(device)

      outputs = model(inputs)

      # Computing loss
      loss = criterion(outputs, target)

      num_correct += (torch.argmax(outputs, axis=1) == target).sum().item()
      loss_value += loss.item()
  
  acc = num_correct / len(eval_dataset)
  #loss_value /= len(eval_dataset)
  return acc, loss_value
class ResBlock(nn.Module):
    def __init__(self, input_channel_size, output_channel_size, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channel_size, output_channel_size, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(output_channel_size)
        self.relu1 = nn.ReLU()  
        self.conv2 = nn.Conv2d(output_channel_size, output_channel_size, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(output_channel_size)
        if stride == 1:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = nn.Conv2d(input_channel_size, output_channel_size, kernel_size=1, stride=stride)
        self.relu2 = nn.ReLU()
    
    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)

        out = self.conv2(out)
        out = self.bn2(out)
        
        shortcut = self.shortcut(x)
        #print(out.shape)
        #print(shortcut.shape)
        
        out = self.relu2(out + shortcut)
        
        return out
 class Res34(nn.Module):
    def __init__(self, in_features, num_classes,feat_dim = 512):
        super().__init__()
        
        self.layers = nn.Sequential(
            nn.Conv2d(in_features, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            ResBlock(64, 64),
            ResBlock(64, 64),
            ResBlock(64, 64),
            ResBlock(64, 128, stride=2),
            ResBlock(128, 128),
            ResBlock(128, 128),
            ResBlock(128, 128),
            ResBlock(128, 256, stride=2),
            ResBlock(256, 256),
            ResBlock(256, 256),
            ResBlock(256, 256),
            ResBlock(256, 256),
            ResBlock(256, 256),
            ResBlock(256, 512, stride=2),
            ResBlock(512, 512),
            ResBlock(512, 512),
            nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1
            nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64
        )
        self.linear = nn.Linear(512, feat_dim)
        self.relu = nn.ReLU(inplace=True)
        self.linear_output = nn.Linear(512, num_classes)        
    def forward(self, x, return_embedding=False):
        embedding = self.layers(x) 
        embedding_out = self.relu(self.linear(embedding))
        output = self.linear_output(embedding)
        if return_embedding:
            return embedding,output
        else:
            return output  
numEpochs = 1
in_features = 3 # RGB channels

learningRate = 1e-9
weightDecay = 5e-5

num_classes = len(train_dataset.classes)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = Res34(in_features, num_classes)
'''
pretrained_dict = torch.load('/data4/wuyuchen/Best Model CrossEntropyLoss')
pretrained_dict = pretrained_dict['model_state_dict']
model_dict = model.state_dict()
pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k.startswith('layers.18') == False}
model_dict.update(pretrained_dict)
model.load_state_dict(model_dict)
'''
model.load_state_dict(base_model_state_dict)
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor = 0.5, patience=5)
best_acc = 0.0
for epoch in range(1):
  start_time = time.time()
  train_loss = train_model(train_dataloader, model)
  end_time = time.time()
  # Validation'''
  start_time = time.time()
  val_acc, val_loss = evaluate_model(eval_dataloader, model)
  end_time = time.time()
  print("Epoch: "+str(epoch)+", Time taken:"+str(end_time-start_time)+" sec, Validation loss: "+str(val_loss)+", Validation accuracy: "+str(val_acc*100)+"%")
  #Saving best Model
  if val_acc > best_acc:
    best_acc = val_acc
  scheduler.step(val_acc)
